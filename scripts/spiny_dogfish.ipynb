{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions for model \n",
    "%run model_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from __future__ import print_function, division\n",
    "import sys\n",
    "import numpy as np\n",
    "import random as random\n",
    "import math as math\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from random import triangular\n",
    "import scipy.stats as sst\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import jude_plot_code as plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejection\n"
     ]
    }
   ],
   "source": [
    "# set code preferences for spiny dogfish model\n",
    "year_split = 2013 # in what year should the data be split into estimation (below year_split) or validation (equals to or after year_split)?\n",
    "abc_options = ['regression','rejection']\n",
    "abc_pref = abc_options[1] # choose which approach to Approximate Bayesian Computation to use \n",
    "print(abc_pref) # check "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haulid',\n",
       " 'lengthclass',\n",
       " 'spp',\n",
       " 'numlengthclass',\n",
       " 'region',\n",
       " 'year',\n",
       " 'wtcpue',\n",
       " 'common',\n",
       " 'stratum',\n",
       " 'stratumarea',\n",
       " 'lat',\n",
       " 'lon',\n",
       " 'depth']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import spiny dogfish data & look at columns \n",
    "dat_trawl = pd.read_csv(\"../processed-data/dogfish_prepped_data.csv\")\n",
    "list(dat_trawl.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only needed columns\n",
    "dat_trawl = dat_trawl[['haulid','numlengthclass', 'year', 'lat','lengthclass']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['haulid', 'temp_surface', 'temp_bottom']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import ROMS temperature data and look at columns \n",
    "# this is a placeholder for more appropriate data for spiny dogfish that goes back earlier and only has temperature\n",
    "# it's already been matched with the unique NOAA haulIDs for the cod forecast challenge\n",
    "dat_roms = pd.read_csv(\"~/github/SDM-convergence/data/haul_ROMS.csv\", usecols = ['unique_id',  'temp_bottom', 'temp_surface'])\n",
    "dat_roms.rename({\"unique_id\":\"haulid\"},axis=\"columns\",inplace=True)\n",
    "list(dat_roms.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1963\n",
      "2018\n",
      "1963\n",
      "2012\n"
     ]
    }
   ],
   "source": [
    "# filter trawl data \n",
    "dat_estimation = dat_trawl.loc[(dat_trawl['year'] < year_split)] # use years before year_split for estimation\n",
    "\n",
    "# check year filtering worked correctly\n",
    "print(dat_trawl.year.min())\n",
    "print(dat_trawl.year.max())\n",
    "\n",
    "print(dat_estimation.year.min())\n",
    "print(dat_estimation.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round latitudes to integers\n",
    "dat_estimation.lat = dat_estimation.lat.round().astype(np.int) # revisit and be more precise about rounding; currently rounding to nearest integer, so bands are defined as center points (35-degree band runs from 34.51 to 35.49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking care of missing data in both data frames\n",
    "# AF: commenting this out as interpolation is a bit dodgy here. will just deal with NAs \n",
    "# dat_estimation=dat_estimation.interpolate(method ='linear', limit_direction ='forward')\n",
    "# dat_estimation=dat_estimation.interpolate(method ='linear', limit_direction ='backward')\n",
    "# dat_roms =dat_roms.interpolate(method ='linear', limit_direction ='forward')\n",
    "# dat_roms=dat_roms.interpolate(method ='linear', limit_direction ='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge haul and ROMS datasets \n",
    "# because this is an inner join, it will omit NOAA hauls with no ROMS data, and ROMS data with no matches in the species' survey dataframe\n",
    "dat_estimation=pd.merge(dat_estimation, dat_roms, how=\"inner\", on=\"haulid\")\n",
    "list(dat_estimation.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track number of latitudes--currently the spatial unit of analysis\n",
    "Nlat = dat_estimation['lat'].max()-dat_estimation['lat'].min()\n",
    "latrange = np.arange(start=dat_estimation.lat.min(), stop=dat_estimation.lat.max(), step=1)\n",
    "print(Nlat)\n",
    "print(latrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to leave Jude's code as is for now but annotate where I'd like to make changes\n",
    "# here I'm temporarily reversing the changes to more readable object names that I made \n",
    "df=dat_estimation\n",
    "df.rename({\"numlengthclass\":\"NUMLEN\"},axis=\"columns\",inplace=True)\n",
    "#to track the total number of latitudes available\n",
    "nn=df['lat'].max()-df['lat'].min()#15 #AF: add column that preserves true lat value, not just lat index \n",
    "#Libraries to keep track of the patches and stages # AF: make names more intuitive, consider using dataframes instead of dictionaries \n",
    "D={}\n",
    "D1={}\n",
    "\n",
    "#extracting the data from the data frames  and storing according to the patch and stage. We start from the first to the last last patch (1:nn+1)  and when in each patch, we extract the number of species for  each life stage, the temperature for each patch (and compute the avaerage for the year)\n",
    "for q in range(1,nn+2): # nn+2 because range() does not use the final value, i.e., range(1,3) equals [1,2]\n",
    "    #Juveniles for patch 33+q( since the min patch is 34, we will start with patch 34 --to the maximum # AF: get rid of all fixed numerics here \n",
    "    D['J_patch'+ str(q)]=df.loc[(df['lat'] == 33+ q) & (df['lengthclass']=='smalljuv')]\n",
    "    #total number of observations in each patch for each year\n",
    "    n=len(D['J_patch'+ str(q)].year.values)\n",
    "    # the total number of years of data available\n",
    "    m=df['year'].max()-df['year'].min()\n",
    "    #temperature readings when each species was caught in the patch\n",
    "    Abun_TemJ=np.empty((m+1, 3))\n",
    "    kJ=0\n",
    "    kY=0\n",
    "    kA=0\n",
    "    for i in range (0, m+1):\n",
    "        Abun_TemJ[i,0]=kJ\n",
    "        DD=D['J_patch'+ str(q)]\n",
    "        TT=DD.loc[(DD['year'] == 1980+i)]\n",
    "        temp1=DD.loc[(DD['year'] == 1980+i)]\n",
    "     #   print(temp1.temp_bottom.values) # AF: not sure why this is here, just creates a really long return \n",
    "        Abun_TemJ[i,1]=temp1.temp_bottom.values.mean()\n",
    "        Abun_TemJ[i,2]=TT.NUMLEN.values.sum()\n",
    "        \n",
    "        kJ=kJ +1\n",
    "    #After extracting teh temperature and calculating the mean value, we now save it\n",
    "    D1['J_patch'+ str(q)]=Abun_TemJ\n",
    "# now moving to Young Juveniles to perform teh same process as above\n",
    "    D['Y_patch'+ str(q)]=df[(df['lat'] == 33+ q) & (df['lengthclass']=='largejuv')]\n",
    "    n=len(D['Y_patch'+ str(q)].year.values)\n",
    "    m=df['year'].max()-df['year'].min()\n",
    "    Abun_TemY=np.empty((m+1, 3))\n",
    "    for i in range (0, m+1):\n",
    "        Abun_TemY[i,0]=kY\n",
    "        DD=D['Y_patch'+ str(q)]\n",
    "        TT=DD.loc[(DD['year'] == 1980+i)]\n",
    "        temp1=DD.loc[(DD['year'] == 1980+i)]\n",
    "        Abun_TemY[i,1]=temp1.temp_bottom.values.mean()\n",
    "        Abun_TemY[i,2]=TT.NUMLEN.values.sum()\n",
    "        kY=kY +1\n",
    "    D1['Y_patch'+ str(q)]=Abun_TemY\n",
    "#Next we move to Adult and perform teh same as above\n",
    "    D['A_patch'+ str(q)]=df[(df['lat'] == 33+ q) & (df['lengthclass']=='adult')]\n",
    "    Abun_TemA=np.empty((m+1, 3))\n",
    "    for i in range (0, m+1):\n",
    "        Abun_TemA[i,0]=kA\n",
    "        DD=D['A_patch'+ str(q)]\n",
    "        TT=DD.loc[(DD['year'] == 1980+i)]\n",
    "        temp1=DD.loc[(DD['year'] == 1980+i)]\n",
    "        Abun_TemA[i,1]=temp1.temp_bottom.values.mean()\n",
    "        Abun_TemA[i,2]=TT.NUMLEN.values.sum()\n",
    "        kA=kA +1\n",
    "    D1['A_patch'+ str(q)]=Abun_TemA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that data has been structured in a dictionary for the model, run the model\n",
    "# we already imported the model functions at the top\n",
    "#main script starts from here\n",
    "if __name__ == '__main__': # AF: ask Jude what this does \n",
    "    # Import the abundance data and data for the other variables e.g temperature\n",
    "   #  import groundfish_training AF: this isn't necessary anymore and I deleted \"groundfish_training.\" from all the calls to D1 or D\n",
    "    # the total number of generations\n",
    "    T_FINAL = len(D1['J_patch1'][:,0])\n",
    "    #We simulate 20000 sets of parameters for for ABC, using non informatives priors (uniform priors\n",
    "    NUMBER_SIMS = 20000\n",
    "    #no of patches\n",
    "    no_patches=12\n",
    "    rows=T_FINAL\n",
    "    cols=no_patches\n",
    "    # creating an array to store the number of juveniles, young juvenils and adults in each patch\n",
    "    N_J=np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "    N_Y=np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "    N_A=np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "    tempA = np.ndarray(shape=(rows, cols), dtype=float, order='F')\n",
    "    #storing data (secies abundance and temeprature time series data ) in the created arrays\n",
    "    for q in range(1,no_patches+1):\n",
    "        i=q-1\n",
    "        p=q\n",
    "        N_J[:,i]=D1['J_patch'+ str(p)][:,2]\n",
    "        N_Y[:,i]=D1['Y_patch'+ str(p)][:,2]\n",
    "        N_A[:,i]=D1['A_patch'+ str(p)][:,2]\n",
    "        tempA[:,i]=D1['A_patch'+ str(p)][:,1]\n",
    "    #running ABC. See the function for details. returns all the observe summary statitics (OS)and simulated summary statistics (SS) in a matrix with first row corresponding to OS and the rest of the rows to SS as well as the parameter values that led to the simulated summary statistics.\n",
    "    param_save, Obs_Sim         = run_sim()\n",
    "    \n",
    "#normalize the rows of Obs_sim to have NOS in row 1 and NSS in the remaining rows. Substract rows i=2:NUMBER_SIMS from row 1 of Obs_sim (whic contain OS).Compute the eucleadean distance (d) between NSS and NOS then use it along side tolerance (δ), to determine all parameters and NSS corresponding to d ≤ δ.Choose δ such that δ × 100% of the NUMBER_SIMS simulated parameters and NSS are selected. retain the parameters that made this threshold (library), the weights ot be used in local linear regression and the NSS that meets the threshold (stats)\n",
    "    library, dists, stats,stats_SS,  NSS_cutoff, library_index   = sum_stats(Obs_Sim, param_save)\n",
    "# performing rejectio ABC. Note that if UMBER_SIMS is big enough, but rejection and regression ABC leads to teh same results.\n",
    "    result, HPDR=do_rejection(library)\n",
    "    print('see the results below')\n",
    "    print('Estimates from rejection is:', result)\n",
    "    print('Estimated HPDR from rejection is :', HPDR)\n",
    "# Next we have regression ABC, perform it if only you are not performing rejection ABC above. Gives better results for NUMBER_SIMS small. I have commented it.\n",
    "#library_reg=do_logit_transformation(library, param_bound)LJ=34, Ly=68, Linf=200\n",
    "        #result_reg, HPDR_reg=do_kernel_ridge(stats, library_reg, param_bound)\n",
    "    PARAMS1={}\n",
    "    print(result[2])\n",
    "    PARAMS1 = {\"L_0\":result[0] , \"L_inf\": result[1],\"L_J\": 68,\"L_Y\": 34, \"Topt\": result[2], \"width\": result[3], \"kopt\": result[4],\"xi\":result[5], \"m_J\": result[6], \"m_Y\":result[7] , \"m_A\": result[8], \"K\": result[9]}\n",
    "\n",
    "    N_J1, N_Y1, N_A1 = simulation_population(PARAMS1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a file call plot to plot the results.\n",
    "print('i just imported a plot')\n",
    "for q in range(1,no_patches+1):\n",
    "    i=q-1\n",
    "    p=q\n",
    "    plot.do_realdata(N_J1[:,i], N_J[:,i],  'J_abun_rej'+ str(p))\n",
    "    #plot.do_scatter(N_J1[:,i], N_J[:,i],  'J_abun_scatter'+ str(p))\n",
    "    plot.do_realdata(N_Y1[:,i], N_Y[:,i],  'Y_abun_rej'+ str(p))\n",
    "    #plot.do_scatter(N_Y1[:,i], N_Y[:,i],  'Y_abun_scatter'+ str(p))\n",
    "    plot.do_realdata(N_A1[:,i], N_A[:,i],  'A_abun_rej'+ str(p))\n",
    "#plot.do_scatter(N_A1[:,i], N_A[:,i],  'A_abun_scatter'+ str(p))\n",
    "################################################################\n",
    "# plot the figures below if you willl like to plot the heatmap\n",
    "    NJ1=N_J1.transpose()\n",
    "    NJ=N_J.transpose()\n",
    "    NY1=N_Y1.transpose()\n",
    "    NY=N_Y.transpose()\n",
    "    NA1=N_A1.transpose()\n",
    "    NA=N_A.transpose()\n",
    "    print(NJ1.shape)\n",
    "    ax=sns.heatmap(NJ1, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2012)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"sim_J.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "#############################################################\n",
    "    ax = sns.heatmap(NJ, cmap=\"Greys\",  xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2012)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"Obs_J.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "##########################################################\n",
    "    ax = sns.heatmap(NY1, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"Sim_Y.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "##########################################################\n",
    "    ax = sns.heatmap(NY, cmap=\"Greys\", xticklabels=True, yticklabels=True,  cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"Obs_Y.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "############################################################\n",
    "    ax = sns.heatmap(NA1, cmap=\"Greys\", xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"Sim_A.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "############################################################\n",
    "    ax = sns.heatmap(NA, cmap=\"Greys\",  xticklabels=True, yticklabels=True, cbar_kws={'label': 'Abundance'})\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    ax.set_xticklabels(pd.Series(range(1980, 2013)))\n",
    "    ax.set_yticklabels(pd.Series(range(34, 46)))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    ax.figure.savefig(\"Obs_A.png\", bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
