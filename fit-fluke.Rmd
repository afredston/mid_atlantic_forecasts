---
title: "R Notebook"
output: html_document
---

```{r packages}
set.seed(42)
library(tidyverse)
library(tidybayes)
library(Cairo)
library(here)
library(magrittr)
library(rstan)
library(Matrix)
library(ggridges)
library(rstanarm)
```

### Size selectivity

Email from Mark Terceiro at NOAA:

--
The current assessment model provides estimates of NEFSC HB Bigelow survey selectivity at age for 2009-2019.
For spring: age 0 = 0.00, age 1 = 0.75, age 2 = 0.80, age 3-7+ = 1.00 
For fall:      age 0 = 0.72, age 1 = 0.80, age 2 = 1.00, age 3-7+ = 1.00

As a rough estimation of how ages relate to lengths:
In the spring no age 0 fish are caught, age 1 fish are generally < 36 cm, age 2 fish are 36 - 40 cm, and age 3 fish are > 40 cm.
In the fall, age 0 fish are < 30 cm, age 1 fish are 30-40 cm, age 2 fish are 41-45 cm, and age 3 fish are > 45 cm.
--

We used an L50 of 27cm as the stage 2/3 breakpoint, following SAW 66. I haven't revisited whether we should change the breakpoint for stages 1 and 2, currently 18cm (halfway between 27cm and the min length 9cm). We could develop season-specific breakpoints, but if by fall the age 1 fish include some that are reproductive anyway, I think it's fine to start classifying those as stage 3 in our model.

**Size selectivity is already accounted for in generating the prepped fluke data file.**

```{r}
sel_at_stage <- c(0.01,0.1,1) # make this up for now, but assumed selectivity at stage
```

 
### Mortality
 
Stock assessment SAW66 page 20:
Fishing mortality rates and stock sizes were estimated using the ASAP statistical catch at age model. An age-specific instantaneous natural mortality rate providing an average M = 0.25 was assumed for all years. Fishing mortality on the fully selected age 4 fish ranged between 0.744 and 1.622 during 1982-1996 and then decreased to 0.245 in 2007. Since 2007 the fishing mortality rate has increased and was 0.334 in 2017. The 90% confidence interval for F in 2017 was 0.276 to 0.380. 

### Growth

Summer flounder seem to reach reproductive size within three years -- actually, much faster -- so I've made the growth rate 1, indicating that the probability that individuals in a size class transition to the next one in any time step is 100%.

```{r data}
flukedat <- read_csv(here("processed-data","fluke_prepped_data.csv")) %>% 
  filter(year>=1968) %>% 
  mutate(latround = floor(lat))

# keep only lat bands sampled every year
latOK <- flukedat %>% 
  select(latround, year) %>% 
  distinct() %>% 
    group_by(latround) %>% 
  summarise(n=n()) %>% 
  filter(n==max(n)) # NEED TO CHECK THAT THESE ARE CONTINUOUS AND MAKE SENSE

flukedat %<>% filter(latround %in% latOK$latround)

## get time dimension
years <- sort(unique(flukedat$year)) # should we cut out pre-1968?
ny <- length(years)
years_train <- ny-10
years_proj <- 10

flukedat <- flukedat %>%
  filter(latround < 43)
#get other dimensions
patches <- sort(unique(flukedat$latround))
np = length(patches) 
ns=3

# make temperature matrix
sbtdat <- flukedat %>% 
  group_by(latround, year) %>% 
  summarise(sbt=mean(btemp, na.rm=TRUE)) %>% 
  ungroup() %>% 
  mutate(patch= as.integer(as.factor(latround)), 
         year=as.integer(as.factor(year))) %>% #change real values to indices
  select(-latround)

sbt <- as.matrix(with(sbtdat, sparseMatrix(patch, year, x=sbt))) 

# make population matrix
flukepop <- flukedat %>% 
  group_by(year,latround, lengthclass) %>% 
  summarise(count=sum(numlengthclass))%>% 
  ungroup() %>% 
  mutate(patch=as.integer(as.factor(latround)),
         year=as.integer(as.factor(year)),
         stage=recode(lengthclass, smalljuv=1, largejuv=2, adult=3),
         count=floor(count)) %>% #change real values to indices and counts to integers (not sure if the latter is necessary)
  select(-latround, -lengthclass)

pop <- array(NA, dim = c(np, ns, ny)) 
for(p in 1:np){
  for(s in 1:ns){
    for(y in 1:ny){
      tmp <- flukepop %>% filter(patch==p, stage==s, year==y)
      pop[p,s,y] <- tmp$count
    }
  }
}

# mortality
m = 0.25
f = 0.334
z = exp(-m-f)

# growth
g = 1

#spillover
spill=0.1
```

explore data 

```{r explore-data}


  flukepop %>% 
  group_by(patch) %>% 
  mutate(scount = count / max(count)) %>% 
  ungroup() %>% 
    # filter(year > 40, patch == 4) %>% 
  ggplot(aes(stage,scount, fill = factor(patch))) + 
  geom_col(position = "dodge") + 
    facet_wrap(~year)


flukepop %>% 
  group_by(stage) %>% 
  summarise(missing = mean(count == 0))

flukepop %>% 
  group_by(year,patch) %>% 
  summarise(hmm = (count[stage ==3] == 0 & sum(count) > 0))
  
```
# New Model Idea

Use stage 3 to scale the population, fir to the absolute numbers in 3, and back out the recruits. 

Two if statements, if there are any stage comps, and if there are any stage 3s

Let's keep it simple and say that if no stage 3 individuals are observed, nothing is observed, to avoid a weird edge case where some fish are seen, but no stage 3 are seen. Will need to revist this as the data change

So what's the observation model? Probably something like temperature plus a patch fixed effect

```{r}
a <- flukepop %>% 
  group_by(year, patch) %>% 
  summarise(seen = count[stage == 3] > 0,
            numbers = sum(count))

sbt_frame <- sbt %>% 
  as.data.frame() %>% 
  mutate(patch = 1:nrow(.)) %>% 
  pivot_longer(-patch, names_to = "year", values_to = "sbt", names_prefix = "V") %>% 
  mutate(year = as.integer(year))

a <- a %>% 
  left_join(sbt_frame, by = c("year","patch")) %>% 
  group_by(patch) %>% 
  mutate(centered_numbers = numbers - mean(numbers)) %>% 
  group_by(patch) %>% 
  arrange(patch,year) %>% 
  mutate(delta_sbt = sbt - lag(sbt,2),
         delta_numbers = numbers - lag(numbers,1))

mean(a$seen)

a %>% 
  ggplot(aes(sbt, numbers)) + 
  geom_point()

a %>% 
  ggplot(aes(sbt, centered_numbers)) + 
  geom_point()

a %>% 
  ggplot(aes(delta_sbt, delta_numbers)) + 
  geom_point() + 
  scale_x_continuous(name = "Change in SBT") +
  scale_y_continuous(name = "Change in Numbers")

a %>% 
  ggplot(aes(sbt, fill = factor(patch))) + 
  geom_density(alpha = 0.25)

seen_model <- stan_glm(seen ~ sbt, data = a, family = "binomial") # should have patch as well, but let's keep it simple for now, seems to work well enough
# rstanarm::launch_shinystan(seen_model)
summary(seen_model)
plot(seen_model)

rstanarm::posterior_vs_prior(seen_model)


```


At the moment modeling recruit as temperature driven deviations from mean recruitment. Looking at the patches, I don't think that makes much sense. Let's instead make temperature alter mean recruitment itself, and then allow deviation away from that. 

So, lets say that

$$\bar{r}_{p,y} = Re^{f(t_{p,y})} $$

and then

$$r_{p,y} \sim normal(log(\bar{r}_{p,y}),\sigma_r)$$

So, let's say presence / absence of stage 3 individuals is

$$seen_{p,y} \sim bernoulli(1 + sst_{p,y})$$

If anything is seen...

$$1 \sim bernoulli(1 + sst_{p,y})$$

$$n_{s=3,p,y} \sim NB(\hat{n_{s=3,p,y}} \times sel_{s = 3},\phi) $$

$$pn_{p,y} \sim multinomial(\hat{pn_{p,y} \times sel})$$

else

$$0 \sim bernoulli(1 + sst_{p,y})$$



```{r fit T_dep_rec}

stage_data <- list(
  sel_at_stage = sel_at_stage,
  sst = sbt[, 1:years_train], # this isn't SST anymore, it's SBT... need to rename in model
  sst_proj = sbt[, (years_train):ny],
  spill = spill,
  np = np,
  ns = ns,
  ny = years_train,
  n_p_s_y = pop[,,1:years_train],
  proj_init = pop[,,years_train],
  mean_length_at_age = 1:ns,
  ny_proj = years_proj + 1, 
  m = z,  # should rename model to Z at some point?
  g = g
)

warmups <- 2000

total_iterations <- 4000

max_treedepth <-  12

n_chains <-  4

n_cores <- 4 

stage_model_fit <- stan(file = here::here("src","T_dep_rec_fluke.stan"), # check that it's the right model!
           data = stage_data,
           chains = 4,
           warmup = warmups,
           iter = total_iterations,
           cores = 4,
           refresh = 250,
           control = list(max_treedepth = max_treedepth,
                          adapt_delta = 0.95)
           )

```

Process results
```{r}

n_p_s_y_hat <-  tidybayes::gather_draws(stage_model_fit, n_p_s_y_hat[patch, stage,year], n = 1000)

n_p_s_y <- pop[,,1:years_train] %>% 
  reshape::melt(varnames = c("patch","stage","year")) %>% 
  as_tibble() 

sel <- tibble(stage = 1:ns, sel = sel_at_stage)

gg_n_p_s_y_hat <- n_p_s_y_hat %>% 
  left_join(sel, by = "stage") %>% 
  ggplot() +
  stat_lineribbon(aes(x = year, y = .value * sel, group=stage),.width = c(.99, .95, .8, .5), color = "red") +
  geom_point(data = n_p_s_y, aes(x=year, y=value, group=stage), size = 2,alpha = 0.5) + 
  facet_grid(patch~stage, scales = "free_y") + 
  scale_fill_brewer() +
  labs(title="model predictions")

gg_n_p_s_y_hat
gg_n_p_s_y_hat_s3 <- n_p_s_y_hat %>% 
  left_join(sel, by = "stage") %>% 
  filter(stage == 3) %>% 
  ggplot() +
  stat_lineribbon(aes(x = year, y = .value * sel, group=stage),.width = c(.99, .95, .8, .5), color = "red") +
  geom_point(data = n_p_s_y %>% filter(stage == 3), aes(x=year, y=value, group=stage), size = 2,alpha = 0.5) + 
  facet_wrap(~patch) + 
  scale_fill_brewer() +
  labs(title="model predictions")

gg_n_p_s_y_hat_s3
```


```{r}


pp_proj_n_p_s_y_hat <-  tidybayes::gather_draws(stage_model_fit, pp_proj_n_p_s_y_hat[patch, stage,year], n = 1000)

n_p_s_y_proj <- pop[,,(years_train):ny] %>% 
  reshape::melt(varnames = c("patch","stage","year")) %>% 
  as_tibble() 


gg_pp_proj_n_p_s_y_hat <- pp_proj_n_p_s_y_hat %>% 
  filter(stage == 3) %>% 
  ggplot() +
  stat_lineribbon(aes(x = year, y = .value, group=stage),.width = c(.99, .95, .8, .5), color = "red") +
  geom_point(data = n_p_s_y_proj %>% filter(stage == 3), aes(x=year, y=value, group=stage), size = 2,alpha = 0.5) + 
  facet_wrap(~patch, scales = "free_y") + 
  scale_fill_brewer() +
  labs(title="forecast")

gg_pp_proj_n_p_s_y_hat
```

